{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "csv_file_path = '/content/final_data.csv'\n",
        "data = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Split the data into a 60:40 ratio\n",
        "train_data, test_data = train_test_split(data, test_size=0.5, random_state=42)\n",
        "\n",
        "# Save the split data into new CSV files\n",
        "train_data.to_csv('train_data.csv', index=False)\n",
        "test_data.to_csv('test_data.csv', index=False)\n"
      ],
      "metadata": {
        "id": "5sVe9qRVmzGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the testing data CSV file into a DataFrame\n",
        "test_data = pd.read_csv('test_data.csv')\n",
        "\n",
        "# Split the testing data into labeled (50%) and unlabeled (50%) parts\n",
        "labeled_data, unlabeled_data = train_test_split(test_data, test_size=0.5, random_state=42)\n",
        "\n",
        "# Save the labeled and unlabeled data into separate CSV files\n",
        "labeled_data.to_csv('labeled_data.csv', index=False)\n",
        "unlabeled_data.drop(columns=['Label'], inplace=True)  # Drop the label column\n",
        "unlabeled_data.to_csv('unlabeled_data.csv', index=False)\n"
      ],
      "metadata": {
        "id": "5Sf0lJvym73d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adTnbko0l7oL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import re\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dropout, Dense, Concatenate, BatchNormalization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGXza1YUx0Eg",
        "outputId": "88b26653-65ae-4bca-fe01-a1fe14cd0eed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "71590\n",
            "<class 'pandas.core.series.Series'>\n",
            "71590\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "\n",
        "# Load data\n",
        "data = pd.read_csv('/content/train_data.csv')\n",
        "\n",
        "# Extract URL strings and labels\n",
        "X = data[['URL','Content-Type','Payload']]\n",
        "y = data['Label']\n",
        "\n",
        "\n",
        "X = X.astype(str)\n",
        "print(type(X))\n",
        "print(len(X))\n",
        "Y = y.astype(str)\n",
        "print(type(Y))\n",
        "print(len(Y))\n",
        "special_characters = [\"'\", '\"', '<', '>', '+', '-', '_', '*', '=', '{', '}', '(', ')', '[', ']', 'âˆ¼', '/',\n",
        "                      '\\\\', '#', ':', ';', '?', '!', '&', '@']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKDfRW-Ex6xi"
      },
      "outputs": [],
      "source": [
        "\n",
        "tokenized_payloads = []\n",
        "\n",
        "def tokenize_and_segment(X):\n",
        "    for payload in X:\n",
        "        segmented_payload = []\n",
        "        current_segment = \"\"\n",
        "\n",
        "        for char in payload:\n",
        "            if char in special_characters:\n",
        "                if current_segment:\n",
        "                    segmented_payload.append(current_segment)\n",
        "                current_segment = \"\"\n",
        "                segmented_payload.append(char)\n",
        "            else:\n",
        "                current_segment += char\n",
        "\n",
        "        if current_segment:\n",
        "            segmented_payload.append(current_segment)\n",
        "\n",
        "        tokenized_payloads.append(segmented_payload)\n",
        "\n",
        "    return tokenized_payloads\n",
        "\n",
        "# Tokenize the payloads\n",
        "tokenized_payloads = tokenize_and_segment(X['Payload'])\n",
        "vocabulary = {}\n",
        "index = 0\n",
        "for sample in tokenized_payloads:\n",
        "    for token in sample:\n",
        "        if token not in vocabulary:\n",
        "            vocabulary[token] = index\n",
        "            index += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AM-ALcqx_Ab",
        "outputId": "fac7156c-675d-4df1-ce95-ca4a33e5902a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "71590\n",
            "[[  0   0   0 ...   0   0   0]\n",
            " [  0   0   0 ...   0   0   0]\n",
            " [  1   2   3 ...   0   0   0]\n",
            " ...\n",
            " [  0   0   0 ...   0   0   0]\n",
            " [  0   0   0 ...   0   0   0]\n",
            " [107   5   2 ...   0   0   0]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Step 4: Sequence Padding/Truncation\n",
        "max_sequence_length = 498 # Adjust this value based on your requirements\n",
        "\n",
        "# Convert tokens to indices based on vocabulary\n",
        "sequences = [[vocabulary[token] for token in sample] for sample in tokenized_payloads]\n",
        "\n",
        "# Pad sequences to a maximum length/\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "# Print the padded sequences\n",
        "print(len(sequences))\n",
        "print(padded_sequences)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "009Ncb7NyC6q",
        "outputId": "507a6e8b-1735-472b-b9f9-fb808b83ac24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "71590\n",
            "71590\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Initialize a Tokenizer\n",
        "tokenizer = Tokenizer(filters=special_characters, oov_token='<OOV>')\n",
        "\n",
        "# Fit the tokenizer on payload data to build the vocabulary\n",
        "tokenizer.fit_on_texts(X['Payload'])\n",
        "\n",
        "# Convert text to sequences of indices\n",
        "sequences = tokenizer.texts_to_sequences(X['Payload'])\n",
        "\n",
        "# Pad sequences to a maximum length\n",
        "max_sequence_length = 498\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "# Print the padded sequences\n",
        "print(len(sequences))\n",
        "print(len(padded_sequences))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AGtIpYgyeSx"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "# from tensorflow.keras.models import Model\n",
        "# from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dropout, Dense, Concatenate, BatchNormalization\n",
        "# from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# # Load data\n",
        "# data = pd.read_csv('/content/final_data.csv',encoding='UTF-8')\n",
        "\n",
        "# # Extract URL strings and labels\n",
        "# X = data[['URL', 'Content-Type', 'Payload']]\n",
        "# y = data['Label']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3_jqD7myqOH",
        "outputId": "662ab28e-5522-4b99-d979-5eddfc9edd96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "71590\n",
            "<class 'pandas.core.series.Series'>\n",
            "71590\n"
          ]
        }
      ],
      "source": [
        "X = X.astype(str)\n",
        "print(type(X))\n",
        "print(len(X))\n",
        "Y = y.astype(str)\n",
        "print(type(Y))\n",
        "print(len(Y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdpxZWfVyg5V"
      },
      "outputs": [],
      "source": [
        "# Tokenize and pad sequences\n",
        "max_sequence_length = 100\n",
        "tokenizer = Tokenizer(filters=[\"'\", '\"', '<', '>', '+', '-', '_', '*', '=', '{', '}', '(', ')', '[', ']', 'âˆ¼', '/',\n",
        "                      '\\\\', '#', ':', ';', '?', '!', '&', '@'], oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(X['Payload'])\n",
        "sequences = tokenizer.texts_to_sequences(X['Payload'])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(y)\n",
        "one_hot_labels = to_categorical(encoded_labels)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtfrLhk3yk1G"
      },
      "outputs": [],
      "source": [
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, one_hot_labels, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2IB7BOiyyPp",
        "outputId": "a47ae6d8-111f-46e8-dfdc-366f208b323a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 100)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, 100, 128)     6053888     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 98, 64)       24640       ['embedding_1[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 97, 64)       32832       ['embedding_1[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)              (None, 96, 64)       41024       ['embedding_1[0][0]']            \n",
            "                                                                                                  \n",
            " global_max_pooling1d_3 (Global  (None, 64)          0           ['conv1d_3[0][0]']               \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " global_max_pooling1d_4 (Global  (None, 64)          0           ['conv1d_4[0][0]']               \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " global_max_pooling1d_5 (Global  (None, 64)          0           ['conv1d_5[0][0]']               \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 192)          0           ['global_max_pooling1d_3[0][0]', \n",
            "                                                                  'global_max_pooling1d_4[0][0]', \n",
            "                                                                  'global_max_pooling1d_5[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 192)          0           ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 64)           12352       ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 6)            390         ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 6,165,126\n",
            "Trainable params: 6,165,126\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/5\n",
            "895/895 [==============================] - 76s 83ms/step - loss: 0.7830 - accuracy: 0.6470 - val_loss: 0.7652 - val_accuracy: 0.6432\n",
            "Epoch 2/5\n",
            "895/895 [==============================] - 19s 21ms/step - loss: 0.7590 - accuracy: 0.6491 - val_loss: 0.7659 - val_accuracy: 0.6431\n",
            "Epoch 3/5\n",
            "895/895 [==============================] - 11s 12ms/step - loss: 0.7559 - accuracy: 0.6495 - val_loss: 0.7667 - val_accuracy: 0.6430\n",
            "Epoch 4/5\n",
            "895/895 [==============================] - 11s 12ms/step - loss: 0.7538 - accuracy: 0.6502 - val_loss: 0.7652 - val_accuracy: 0.6421\n",
            "Epoch 5/5\n",
            "895/895 [==============================] - 11s 12ms/step - loss: 0.7528 - accuracy: 0.6502 - val_loss: 0.7659 - val_accuracy: 0.6427\n",
            "448/448 [==============================] - 1s 3ms/step - loss: 0.7659 - accuracy: 0.6427\n",
            "Test Loss: 0.7659\n",
            "Test Accuracy: 0.6427\n"
          ]
        }
      ],
      "source": [
        "# TextCNN model\n",
        "input_layer = Input(shape=(max_sequence_length,))\n",
        "embedding_layer = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128)(input_layer)\n",
        "\n",
        "conv_layers = []\n",
        "filter_sizes = [3, 4, 5]\n",
        "for filter_size in filter_sizes:\n",
        "    conv = Conv1D(filters=64, kernel_size=filter_size, activation='relu')(embedding_layer)\n",
        "    pool = GlobalMaxPooling1D()(conv)\n",
        "    conv_layers.append(pool)\n",
        "\n",
        "concatenated = Concatenate()(conv_layers)\n",
        "dropout = Dropout(0.5)(concatenated)\n",
        "dense = Dense(64, activation='relu')(dropout)\n",
        "output_layer = Dense(len(label_encoder.classes_), activation='softmax')(dense)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=64)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss:.4f}')\n",
        "print(f'Test Accuracy: {accuracy:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FPGbPQ70tFO"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "# Assuming you have a trained model stored in the 'model' variable\n",
        "model.save('trained_model.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIhsH-I1fgsF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import csv\n",
        "\n",
        "def apply_keyword_avoidance_perturbation(original_sample, keyword_library):\n",
        "    def randomize_number(n):\n",
        "        return str(np.random.randint(10))\n",
        "\n",
        "    def randomize_letter(c):\n",
        "        if c.isalpha():\n",
        "            return chr(np.random.randint(ord('a'), ord('z') + 1))\n",
        "        return c\n",
        "\n",
        "    perturbed_words = []\n",
        "    for word in original_sample:\n",
        "        if word in keyword_library:\n",
        "            perturbed_words.append(word)\n",
        "        else:\n",
        "            special_characters = ['\\'', '\"', '<', '>', '+', '-', '_', '*', '=', '{', '}', '(', ')', '[', ']', 'âˆ¼', '/',\n",
        "                                  '\\\\', '#', ':', ';', '?', '!', '&', '@', '%']\n",
        "            if any(char in special_characters for char in word):\n",
        "                perturbed_words.append(word)\n",
        "            else:\n",
        "                perturbed_word = ''.join(randomize_number(c) if c.isdigit() else randomize_letter(c) for c in word)\n",
        "                perturbed_words.append(perturbed_word)\n",
        "\n",
        "    return perturbed_words\n",
        "\n",
        "# Define the keyword library\n",
        "keyword_library = {\"login\", \"password\", \"malicious\", \"attack\", \"exploit\", \"hacker\", \"virus\",\n",
        "                   \"SELECT\", \"UNION\", \"AND\", \"http\", \"script\"}\n",
        "\n",
        "# Example usage of data augmentation with payload\n",
        "tokenized_payloads = tokenized_payloads  # Your list of tokenized payloads\n",
        "\n",
        "augmented_payloads = []\n",
        "for original_payload in tokenized_payloads:\n",
        "    augmented_payload = apply_keyword_avoidance_perturbation(original_payload, keyword_library)\n",
        "    augmented_payloads.append(augmented_payload)\n",
        "\n",
        "# Save the results to a CSV file\n",
        "csv_file = \"augmented_payloads.csv\"\n",
        "\n",
        "with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Original Payload\", \"Augmented Payload\"])  # Write header\n",
        "\n",
        "    for original_payload, augmented_payload in zip(tokenized_payloads, augmented_payloads):\n",
        "        writer.writerow([original_payload, augmented_payload])\n",
        "\n",
        "print(f\"Results saved to {csv_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mr_Rxt8YlMIj"
      },
      "outputs": [],
      "source": [
        "\n",
        "url = data['URL'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gssZPXREevs6",
        "outputId": "605a991a-798c-430e-9534-858c0b37d52f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to augmented_urls.csv\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "def apply_keyword_avoidance_perturbation(original_url, keyword_library):\n",
        "    def randomize_number(n):\n",
        "        return str(np.random.randint(10))\n",
        "\n",
        "    def randomize_letter(c):\n",
        "        if c.isalpha():\n",
        "            return chr(np.random.randint(ord('a'), ord('z') + 1))\n",
        "        return c\n",
        "\n",
        "    components = re.split(r'(/|=|\\?|&|\\.)', original_url)  # Split using regex\n",
        "    perturbed_components = []\n",
        "\n",
        "    for component in components:\n",
        "        if component in ('/', '=', '.', \"&\", \"?\"):\n",
        "            perturbed_components.append(component)\n",
        "            continue\n",
        "\n",
        "        if any(keyword in component for keyword in keyword_library):\n",
        "            perturbed_components.append(component)\n",
        "        else:\n",
        "            perturbed_component = ''.join(randomize_number(c) if c.isdigit() else randomize_letter(c) for c in component)\n",
        "            perturbed_components.append(perturbed_component)\n",
        "\n",
        "    perturbed_url = ''.join(perturbed_components)\n",
        "    return perturbed_url\n",
        "\n",
        "# Example DataFrame with URLs\n",
        "# data = pd.read_csv('your_data.csv')  # Replace with the actual file name\n",
        "url = data['URL'].tolist()\n",
        "\n",
        "# Define the keyword library\n",
        "keyword_library = {\"login\", \"user\", \"password\", \"malicious\", \"attack\", \"exploit\", \"hacker\", \"virus\",\n",
        "                   \"com\", \"co\", \"cn\", \"in\", \"http\", \"php\"}\n",
        "\n",
        "# Apply data augmentation to each URL in the DataFrame\n",
        "augmented_urls = []\n",
        "for original_url in url:\n",
        "    if isinstance(original_url, str):  # Check for NaN values\n",
        "        augmented_url = apply_keyword_avoidance_perturbation(original_url, keyword_library)\n",
        "        augmented_urls.append(augmented_url)\n",
        "\n",
        "# Save the results to a CSV file\n",
        "csv_file = \"augmented_urls.csv\"\n",
        "\n",
        "with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Original url\", \"Augmented url\"])  # Write header\n",
        "\n",
        "    for original_url, augmented_url in zip(url, augmented_urls):\n",
        "        if isinstance(original_url, str):\n",
        "            writer.writerow([original_url, augmented_url])\n",
        "\n",
        "print(f\"Results saved to {csv_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgqTohiZeBa-",
        "outputId": "b19294ff-fae7-478a-a3e7-98e4848371c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['www.google-analytics.com/analytics.js', 'ipv6.msftconnecttest.com/connecttest.txt', 'www.netdania.com/jolokia/list', 'www.netdania.com/', 'www.netdania.com/']\n"
          ]
        }
      ],
      "source": [
        "X = data[['URL', 'Content-Type', 'Payload']]\n",
        "url_list = X['URL'].tolist()\n",
        "\n",
        "print(url_list[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFZ2hmPNBeAp"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import csv\n",
        "\n",
        "def perform_data_augment(payload):\n",
        "    # Randomly choose one augmentation technique\n",
        "    augmentation_technique = random.choice([\"insertion\", \"deletion\", \"substitution\"])\n",
        "\n",
        "    if augmentation_technique == \"insertion\":\n",
        "        # Perform insertion augmentation\n",
        "        augmented_payload = payload + random.choice([\"ADD\", \"<script>\", \"</script>\", \"SELECT\"])\n",
        "    elif augmentation_technique == \"deletion\":\n",
        "        # Perform deletion augmentation\n",
        "        augmented_payload = payload[:-1]\n",
        "    else:\n",
        "        # Perform substitution augmentation\n",
        "        random_index = random.randint(0, len(payload)-1)\n",
        "        augmented_payload = payload[:random_index]  + payload[random_index+1:]\n",
        "\n",
        "    return augmented_payload\n",
        "\n",
        "def generate_augment_csv(input_csv_path, output_csv_path):\n",
        "    with open(input_csv_path, 'r') as input_file, open(output_csv_path, 'w', newline='') as output_file:\n",
        "        reader = csv.reader(input_file)\n",
        "        writer = csv.writer(output_file)\n",
        "\n",
        "        header = next(reader)\n",
        "        writer.writerow(header + [\"Augmented Payload\"])\n",
        "\n",
        "        for row in reader:\n",
        "            payload = row[0]\n",
        "            augmented_payload = perform_data_augment(payload)\n",
        "            writer.writerow(row + [augmented_payload])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_csv_path = \"target.csv\"  # Provide the path to your input CSV file\n",
        "    GDTSL = \"GDTSL.csv\"  # Provide the desired path for the output CSV file\n",
        "    generate_augment_csv(input_csv_path, GDTSL)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qec1VyaXboRw"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import csv\n",
        "\n",
        "def perform_data_augment(payload):\n",
        "    # Randomly choose one augmentation technique\n",
        "    augmentation_technique = random.choice([\"insertion\", \"deletion\", \"substitution\"])\n",
        "\n",
        "    if augmentation_technique == \"insertion\":\n",
        "        # Perform insertion augmentation\n",
        "        augmented_payload = payload + random.choice([\"ADD\", \"<script>\", \"</script>\", \"SELECT\"])\n",
        "    elif augmentation_technique == \"deletion\":\n",
        "        # Perform deletion augmentation\n",
        "        augmented_payload = payload[:-1]\n",
        "    else:\n",
        "        # Perform substitution augmentation\n",
        "        random_index = random.choice(range(len(payload)))\n",
        "        augmented_payload = payload[:random_index]  + payload[random_index+1:]\n",
        "\n",
        "    return augmented_payload\n",
        "\n",
        "def generate_augment_csv(input_csv_path, output_csv_path):\n",
        "    with open(input_csv_path, 'r') as input_file, open(output_csv_path, 'w', newline='') as output_file:\n",
        "        reader = csv.reader(input_file)\n",
        "        writer = csv.writer(output_file)\n",
        "\n",
        "        header = next(reader)\n",
        "        writer.writerow(header + [\"Augmented Payload\"])\n",
        "\n",
        "        for row in reader:\n",
        "            payload = row[0]\n",
        "            augmented_payload = perform_data_augment(payload)\n",
        "            writer.writerow(row + [augmented_payload])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_csv_path = \"/content/unlabelled.csv\"  # Provide the path to your input CSV file\n",
        "    GDTSL = \"GDTSU.csv\"  # Provide the desired path for the output CSV file\n",
        "    generate_augment_csv(input_csv_path, GDTSL)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v57Z7FLPRTIR"
      },
      "outputs": [],
      "source": [
        "DTSL = pd.read_csv('/content/target.csv')\n",
        "DTSU = pd.read_csv('/content/unlabelled.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Sx_X8FpRZ7C"
      },
      "outputs": [],
      "source": [
        "GDTSL = pd.read_csv('/content/GDTSL.csv')\n",
        "GDTSU = pd.read_csv('/content/GDTSU.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# # Assuming DTSU and GDTSL are DataFrame objects\n",
        "# shape = DTSU.shape\n",
        "# DTSU_array = DTSU.to_numpy().reshape(shape)\n",
        "# GDTSU_array = GDTSU.to_numpy().reshape(shape)\n",
        "\n",
        "# # Create new DataFrame objects with reshaped arrays\n",
        "# DTSU = pd.DataFrame(DTSU_array, columns=DTSU.columns, index=DTSU.index)\n",
        "# GDTSU = pd.DataFrame(GDTSU_array, columns=GDTSL.columns, index=GDTSL.index)\n"
      ],
      "metadata": {
        "id": "HXwjVBvMIKuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rows_to_pad = DTSL.shape[0] - DTSU.shape[0]\n",
        "# dummy_rows = pd.DataFrame(np.nan, index=range(rows_to_pad), columns=DTSU.columns)\n",
        "# DTSU = pd.concat([DTSU, dummy_rows])"
      ],
      "metadata": {
        "id": "pyto1uUYMXNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vmx1T3rRj2y"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Step 4: Transfer Semi-Supervised Learning\n",
        "def transfer_semi_supervised_learning(initial_model, DTSL, DTSU, GDTSL, GDTSU, n, Î», num_epochs):\n",
        "    # Initialize the TextCNN classifier of the target system (TM)\n",
        "    TM = tf.keras.models.clone_model(initial_model)\n",
        "    TM.set_weights(initial_model.get_weights())\n",
        "\n",
        "    # Freeze the first n layers of TM\n",
        "    for layer in TM.layers[:n]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Define the loss function and optimizer\n",
        "    loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "    # Generate pseudo labeled samples using the keyword avoidance perturbation mechanism\n",
        "    pseudo_labeled_samples = []\n",
        "    for sample in DTSL:\n",
        "       # pseudo_labeled_samples.append(apply_keyword_avoidance_perturbation(sample, keyword_library))\n",
        "        # pseudo_labeled_samples = np.reshape(pseudo_labeled_samples, (len(pseudo_labeled_samples), 1))\n",
        "        pseudo_labeled_samples = np.reshape(pseudo_labeled_samples, (len(pseudo_labeled_samples), 7))\n",
        "\n",
        "    # Combine the pseudo labeled and original labeled samples to form a new training set\n",
        "    #combined_samples = np.concatenate((DTSL, np.array(pseudo_labeled_samples)), axis=0)\n",
        "    combined_samples = np.stack((DTSL, DTSU, GDTSL, GDTSU), axis=0)\n",
        "    combined_labels = np.concatenate((np.ones(len(DTSL)), np.zeros(len(pseudo_labeled_samples))), axis=0)\n",
        "\n",
        "    # Train TM on the combined dataset\n",
        "    for epoch in range(num_epochs):\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred_combined = TM(combined_samples, training=True)\n",
        "            loss_combined = loss_fn(combined_labels, y_pred_combined)\n",
        "\n",
        "            # Add the consistency loss using the unlabeled samples\n",
        "            # Consistency loss = Î» * MSE(y_pred_combined, y_pred_unlabeled)\n",
        "            # where y_pred_unlabeled is the prediction on the unlabeled dataset using TM\n",
        "            y_pred_unlabeled = TM(DTSU, training=True)\n",
        "            consistency_loss = Î» * tf.keras.losses.MeanSquaredError()(y_pred_combined, y_pred_unlabeled)\n",
        "\n",
        "            total_loss = loss_combined + consistency_loss\n",
        "\n",
        "        gradients = tape.gradient(total_loss, TM.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, TM.trainable_variables))\n",
        "\n",
        "    return TM\n",
        "\n",
        "# Hyperparameters for transfer semi-supervised learning\n",
        "n = 5  # Number of frozen layers\n",
        "Î» = 0.1  # Trade-off weight for consistency loss\n",
        "num_epochs = 100  # Number of training epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPrDyJ4GRxRx",
        "outputId": "b9351d4e-daed-4543-dbcd-b6445f3fa214"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.series.Series'>\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'pandas.core.frame.DataFrame'>\n"
          ]
        }
      ],
      "source": [
        "# Assuming you have defined the required data and initial_model\n",
        "print(type(DTSL[0]))\n",
        "print(type(DTSU))\n",
        "print(type(GDTSL))\n",
        "#TM = transfer_semi_supervised_learning(model, DTSL, DTSU, GDTSL, GDTSU, n, Î», num_epochs)\n",
        "#TM.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transfer Learning**"
      ],
      "metadata": {
        "id": "ziZWxRAYa5yT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCcEMSzqSC3b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dropout, Dense, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data from various sources\n",
        "labelled_data = pd.read_csv('/content/labeled_data.csv', encoding='UTF-8')\n",
        "unlabelled_data = pd.read_csv('/content/unlabeled_data.csv', encoding='UTF-8')\n",
        "# generated_labelled_data = pd.read_csv('/content/Generated_Labelled.csv', encoding='UTF-8')\n",
        "# generated_unlabelled_data = pd.read_csv('/content/Generated_Unlabelled.csv', encoding='UTF-8')\n",
        "\n"
      ],
      "metadata": {
        "id": "nzsnbB_1bE2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate labelled and generated labelled data\n",
        "combined_labelled_data = pd.concat([labelled_data], ignore_index=True)\n",
        "\n",
        "# Combine unlabelled and generated unlabelled data\n",
        "combined_unlabelled_data = pd.concat([unlabelled_data], ignore_index=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "Eyth7uDsbHuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_labelled = combined_labelled_data[['URL','Content-Type','Payload']]\n",
        "y_labelled = combined_labelled_data['Label']\n",
        "\n",
        "X_unlabelled = combined_unlabelled_data[['URL','Content-Type','Payload']]\n",
        "\n"
      ],
      "metadata": {
        "id": "ZZ9ef9VsbK3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_labelled = X_labelled.astype(str)\n",
        "print(type(X_labelled))\n",
        "print(len(X_labelled))\n",
        "y_labelled = y_labelled.astype(str)\n",
        "print(type(y_labelled))\n",
        "print(len(y_labelled))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-ozsFMJop78",
        "outputId": "34be9950-3a45-45a2-de0f-53508e7d9ac2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "23863\n",
            "<class 'pandas.core.series.Series'>\n",
            "23863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and pad sequences for labelled data\n",
        "max_sequence_length = 100\n",
        "tokenizer = Tokenizer(filters=[\"'\", '\"', '<', '>', '+', '-', '_', '*', '=', '{', '}', '(', ')', '[', ']', 'âˆ¼', '/',\n",
        "                      '\\\\', '#', ':', ';', '?', '!', '&', '@'], oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(X_labelled['Payload'])\n",
        "sequences = tokenizer.texts_to_sequences(X_labelled['Payload'])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post', truncating='post')"
      ],
      "metadata": {
        "id": "EjaUDSL0bNyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert labels to one-hot encoding\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(y_labelled)\n",
        "one_hot_labels = to_categorical(encoded_labels)\n"
      ],
      "metadata": {
        "id": "duzI02mwbRlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Split data into train and test sets for labelled data\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, one_hot_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenize and pad sequences for unlabelled data\n",
        "sequences_unlabelled = tokenizer.texts_to_sequences(X_unlabelled['Payload'])\n",
        "padded_sequences_unlabelled = pad_sequences(sequences_unlabelled, maxlen=max_sequence_length, padding='post', truncating='post')\n"
      ],
      "metadata": {
        "id": "GBob8y7ubZIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "#from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Assuming X_unlabelled['Payload'] contains your unlabelled data\n",
        "# Replace NaN values with empty strings and convert to strings\n",
        "X_unlabelled['Payload'] = X_unlabelled['Payload'].fillna('').astype(str)\n",
        "\n",
        "# Tokenize and pad sequences for unlabelled data\n",
        "sequences_unlabelled = tokenizer.texts_to_sequences(X_unlabelled['Payload'])\n",
        "padded_sequences_unlabelled = pad_sequences(sequences_unlabelled, maxlen=max_sequence_length, padding='post', truncating='post')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-s33WImpmnw",
        "outputId": "f78860aa-b74f-4598-ab37-f9cccefd8138"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-111-da5f80e2154a>:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_unlabelled['Payload'] = X_unlabelled['Payload'].fillna('').astype(str)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the pre-trained TextCNN model\n",
        "pretrained_model = load_model('trained_model.h5')\n",
        "\n",
        "# Remove the last layer (output layer) from the pre-trained model\n",
        "pretrained_model.layers.pop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiGakcOtbUdC",
        "outputId": "dbeeaa32-3d06-455a-c826-4fc4c62632d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.layers.core.dense.Dense at 0x78f230fee0e0>"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a new output layer for the target domain\n",
        "new_output_layer = Dense(len(label_encoder.classes_), activation='softmax')(pretrained_model.layers[-1].output)\n",
        "\n",
        "# Create the transfer learning model\n",
        "transfer_learning_model = Model(inputs=pretrained_model.input, outputs=new_output_layer)\n",
        "\n"
      ],
      "metadata": {
        "id": "g187N9EQbetN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze the layers of the pre-trained model\n",
        "for layer in transfer_learning_model.layers[:-1]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the transfer learning model\n",
        "transfer_learning_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the transfer learning model on labelled data\n",
        "transfer_learning_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTh3oznwbiSY",
        "outputId": "c270e8a4-a8ea-4774-cf7e-04c069e8ff65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "597/597 [==============================] - 4s 5ms/step - loss: 0.8836 - accuracy: 0.6346 - val_loss: 0.8602 - val_accuracy: 0.6354\n",
            "Epoch 2/10\n",
            "597/597 [==============================] - 3s 4ms/step - loss: 0.8503 - accuracy: 0.6344 - val_loss: 0.8404 - val_accuracy: 0.6352\n",
            "Epoch 3/10\n",
            "597/597 [==============================] - 3s 5ms/step - loss: 0.8372 - accuracy: 0.6342 - val_loss: 0.8304 - val_accuracy: 0.6352\n",
            "Epoch 4/10\n",
            "597/597 [==============================] - 3s 5ms/step - loss: 0.8280 - accuracy: 0.6348 - val_loss: 0.8244 - val_accuracy: 0.6352\n",
            "Epoch 5/10\n",
            "597/597 [==============================] - 3s 4ms/step - loss: 0.8228 - accuracy: 0.6345 - val_loss: 0.8203 - val_accuracy: 0.6352\n",
            "Epoch 6/10\n",
            "597/597 [==============================] - 3s 4ms/step - loss: 0.8201 - accuracy: 0.6348 - val_loss: 0.8174 - val_accuracy: 0.6352\n",
            "Epoch 7/10\n",
            "597/597 [==============================] - 3s 5ms/step - loss: 0.8178 - accuracy: 0.6350 - val_loss: 0.8154 - val_accuracy: 0.6352\n",
            "Epoch 8/10\n",
            "597/597 [==============================] - 3s 5ms/step - loss: 0.8144 - accuracy: 0.6350 - val_loss: 0.8137 - val_accuracy: 0.6352\n",
            "Epoch 9/10\n",
            "597/597 [==============================] - 3s 5ms/step - loss: 0.8161 - accuracy: 0.6339 - val_loss: 0.8125 - val_accuracy: 0.6352\n",
            "Epoch 10/10\n",
            "597/597 [==============================] - 3s 4ms/step - loss: 0.8152 - accuracy: 0.6343 - val_loss: 0.8117 - val_accuracy: 0.6352\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x78f22edacc40>"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Evaluate the transfer learning model on labelled data\n",
        "loss, accuracy = transfer_learning_model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss (Labelled Data): {loss:.4f}')\n",
        "print(f'Test Accuracy (Labelled Data): {accuracy:.4f}')\n",
        "\n",
        "# Predict using the transfer learning model on unlabelled data\n",
        "predictions_unlabelled = transfer_learning_model.predict(padded_sequences_unlabelled)\n",
        "\n",
        "# Save the updated unlabelled data to a CSV file\n",
        "combined_unlabelled_data['Predictions'] = predictions_unlabelled.tolist()\n",
        "combined_unlabelled_data.to_csv('/content/Updated_Unlabelled.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YGtWQcebkPA",
        "outputId": "5d6aee16-1fe8-410d-be96-7a058d930742"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "150/150 [==============================] - 0s 3ms/step - loss: 0.8117 - accuracy: 0.6352\n",
            "Test Loss (Labelled Data): 0.8117\n",
            "Test Accuracy (Labelled Data): 0.6352\n",
            "746/746 [==============================] - 1s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s-cw94tep_7V"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}