{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kahb5sFCvgCk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import re\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dropout, Dense, Concatenate, BatchNormalization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "csv_file_path = '/content/final_data.csv'\n",
        "data = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Split the data into a 60:40 ratio\n",
        "train_data, test_data = train_test_split(data, test_size=0.5, random_state=42)\n",
        "\n",
        "# Save the split data into new CSV files\n",
        "train_data.to_csv('train_data.csv', index=False)\n",
        "test_data.to_csv('test_data.csv', index=False)"
      ],
      "metadata": {
        "id": "f2mhMkImwHC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the testing data CSV file into a DataFrame\n",
        "test_data = pd.read_csv('test_data.csv')\n",
        "\n",
        "# Split the testing data into labeled (50%) and unlabeled (50%) parts\n",
        "labeled_data, unlabeled_data = train_test_split(test_data, test_size=0.5, random_state=42)\n",
        "\n",
        "# Save the labeled and unlabeled data into separate CSV files\n",
        "labeled_data.to_csv('labeled_data.csv', index=False)\n",
        "unlabeled_data.drop(columns=['Label'], inplace=True)  # Drop the label column\n",
        "unlabeled_data.to_csv('unlabeled_data.csv', index=False)"
      ],
      "metadata": {
        "id": "E5Vf-Mwe_hIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the testing data CSV file into a DataFrame\n",
        "test_data = pd.read_csv('test_data.csv')\n",
        "\n",
        "# Split the testing data into labeled (50%) and unlabeled (50%) parts\n",
        "labeled_data, unlabeled_data = train_test_split(test_data, test_size=0.5, random_state=42)\n",
        "\n",
        "# Save the labeled and unlabeled data into separate CSV files\n",
        "labeled_data.to_csv('labeled_data.csv', index=False)\n",
        "unlabeled_data.drop(columns=['Label'], inplace=True)  # Drop the label column\n",
        "unlabeled_data.to_csv('unlabeled_data.csv', index=False)"
      ],
      "metadata": {
        "id": "VaAmRNQoOJmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "\n",
        "# Load data\n",
        "data = pd.read_csv('/content/train_data.csv')\n",
        "\n",
        "# Extract URL strings and labels\n",
        "X = data[['URL','Content-Type','Payload']]\n",
        "y = data['Label']\n",
        "\n",
        "\n",
        "X = X.astype(str)\n",
        "print(type(X))\n",
        "print(len(X))\n",
        "Y = y.astype(str)\n",
        "print(type(Y))\n",
        "print(len(Y))\n"
      ],
      "metadata": {
        "id": "jPm0gEOawXtm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bd79c85-299a-4be0-ef4b-dbab7979ed2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "1315\n",
            "<class 'pandas.core.series.Series'>\n",
            "1315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "special_characters = [\"'\", '\"', '<', '>', '+', '-', '_', '*', '=', '{', '}', '(', ')', '[', ']', 'âˆ¼', '/',\n",
        "                      '\\\\', '#', ':', ';', '?', '!', '&', '@']\n",
        "\n",
        "tokenized_payloads = []\n",
        "\n",
        "def tokenize_and_segment(X):\n",
        "    for payload in X:\n",
        "        segmented_payload = []\n",
        "        current_segment = \"\"\n",
        "\n",
        "        for char in payload:\n",
        "            if char in special_characters:\n",
        "                if current_segment:\n",
        "                    segmented_payload.append(current_segment)\n",
        "                current_segment = \"\"\n",
        "                segmented_payload.append(char)\n",
        "            else:\n",
        "                current_segment += char\n",
        "\n",
        "        if current_segment:\n",
        "            segmented_payload.append(current_segment)\n",
        "\n",
        "        tokenized_payloads.append(segmented_payload)\n",
        "\n",
        "    return tokenized_payloads\n",
        "\n",
        "tokenized_payloads = tokenize_and_segment(X['Payload'])"
      ],
      "metadata": {
        "id": "vsZKmZj_whfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = {}\n",
        "index = 0\n",
        "for sample in tokenized_payloads:\n",
        "    for token in sample:\n",
        "        if token not in vocabulary:\n",
        "            vocabulary[token] = index\n",
        "            index += 1"
      ],
      "metadata": {
        "id": "4i87nR0MxFMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a Tokenizer\n",
        "tokenizer = Tokenizer(filters=special_characters, oov_token='<OOV>')\n",
        "\n",
        "# Fit the tokenizer on payload data to build the vocabulary\n",
        "tokenizer.fit_on_texts(X['Payload'])\n",
        "\n",
        "# Convert text to sequences of indices\n",
        "sequences = tokenizer.texts_to_sequences(X['Payload'])"
      ],
      "metadata": {
        "id": "diOFumBvO3Lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Sequence Padding/Truncation\n",
        "max_sequence_length = 498 # Adjust this value based on your requirements\n",
        "#Convert tokens to indices based on vocabulary\n",
        "sequences = [[vocabulary[token] for token in sample] for sample in tokenized_payloads]\n",
        "# Pad sequences to a maximum length\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post', truncating = 'post')\n",
        "\n",
        "# Print the padded sequences\n",
        "print(len(sequences))\n",
        "print(len(padded_sequences))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ut13Hd7ZxT0W",
        "outputId": "71f3fb85-d94a-472f-eff0-7982ce671ed7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1315\n",
            "1315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vocabulary creation\n",
        "vocab_size = len(vocabulary)\n",
        "print(vocab_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nn9ciRYdxWdD",
        "outputId": "7ca2856e-69b9-45a3-a66e-1c37a9e526ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1602\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Label encoding\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(y)\n",
        "print(len(encoded_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6seaZdvxrVT",
        "outputId": "e847bedf-fea0-4c79-faf2-e92ca60632f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "59658\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, Conv1D, MaxPooling1D, Flatten, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Vocabulary creation\n",
        "vocabulary_size = len(vocabulary)\n",
        "embedding_dim = 100  # Adjust this based on your requirements\n",
        "max_sequence_length = 498  # Adjust this based on your requirements\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "# Step 5: Embedding Layer\n",
        "input_layer = Input(shape=(max_sequence_length,))\n",
        "embedding_layer = Embedding(input_dim=vocabulary_size, output_dim=embedding_dim)(input_layer)\n",
        "\n",
        "# Step 6: Convolutional Layer\n",
        "num_filters = 128  # Adjust this based on your requirements\n",
        "filter_sizes = [3, 4, 5]  # You can experiment with different filter sizes\n",
        "convolutional_layers = []\n",
        "\n",
        "for filter_size in filter_sizes:\n",
        "    conv_layer = Conv1D(filters=num_filters, kernel_size=filter_size, activation='relu')(embedding_layer)\n",
        "    pool_layer = MaxPooling1D(pool_size=max_sequence_length - filter_size + 1)(conv_layer)\n",
        "    convolutional_layers.append(pool_layer)\n",
        "\n",
        "merged_layer = tf.keras.layers.concatenate(convolutional_layers, axis=1)\n",
        "flatten_layer = Flatten()(merged_layer)\n",
        "\n",
        "# Step 7: Fully Connected Layers\n",
        "hidden_units = 128  # Adjust this based on your requirements\n",
        "dense_layer = Dense(units=hidden_units, activation='relu')(flatten_layer)\n",
        "output_layer = Dense(units=num_classes, activation='softmax')(dense_layer)\n",
        "\n",
        "# Create the model\n",
        "text_cnn_model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compile the model\n",
        "text_cnn_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "text_cnn_model.summary()\n",
        "\n",
        "# Train the model\n",
        "batch_size = 64  # Adjust this based on your resources\n",
        "num_epochs = 10  # Adjust this based on your requirements\n",
        "\n",
        "text_cnn_model.fit(padded_sequences, encoded_labels, batch_size=batch_size, epochs=num_epochs, validation_split=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKrdsZgly3_b",
        "outputId": "6ec62f73-f68a-4954-a718-05f81c08f4e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 498)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 498, 100)     4069600     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 496, 128)     38528       ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 495, 128)     51328       ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 494, 128)     64128       ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1D)   (None, 1, 128)       0           ['conv1d[0][0]']                 \n",
            "                                                                                                  \n",
            " max_pooling1d_1 (MaxPooling1D)  (None, 1, 128)      0           ['conv1d_1[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_2 (MaxPooling1D)  (None, 1, 128)      0           ['conv1d_2[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 3, 128)       0           ['max_pooling1d[0][0]',          \n",
            "                                                                  'max_pooling1d_1[0][0]',        \n",
            "                                                                  'max_pooling1d_2[0][0]']        \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 384)          0           ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 128)          49280       ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 6)            774         ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,273,638\n",
            "Trainable params: 4,273,638\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/10\n",
            "746/746 [==============================] - 116s 140ms/step - loss: 0.7770 - accuracy: 0.6469 - val_loss: 0.7509 - val_accuracy: 0.6521\n",
            "Epoch 2/10\n",
            "746/746 [==============================] - 43s 57ms/step - loss: 0.7558 - accuracy: 0.6501 - val_loss: 0.7477 - val_accuracy: 0.6521\n",
            "Epoch 3/10\n",
            "746/746 [==============================] - 33s 44ms/step - loss: 0.7545 - accuracy: 0.6503 - val_loss: 0.7506 - val_accuracy: 0.6519\n",
            "Epoch 4/10\n",
            "746/746 [==============================] - 29s 38ms/step - loss: 0.7535 - accuracy: 0.6504 - val_loss: 0.7472 - val_accuracy: 0.6523\n",
            "Epoch 5/10\n",
            "746/746 [==============================] - 27s 36ms/step - loss: 0.7523 - accuracy: 0.6504 - val_loss: 0.7493 - val_accuracy: 0.6522\n",
            "Epoch 6/10\n",
            "746/746 [==============================] - 26s 34ms/step - loss: 0.7521 - accuracy: 0.6505 - val_loss: 0.7478 - val_accuracy: 0.6521\n",
            "Epoch 7/10\n",
            "746/746 [==============================] - 26s 35ms/step - loss: 0.7512 - accuracy: 0.6505 - val_loss: 0.7468 - val_accuracy: 0.6519\n",
            "Epoch 8/10\n",
            "746/746 [==============================] - 25s 34ms/step - loss: 0.7514 - accuracy: 0.6505 - val_loss: 0.7503 - val_accuracy: 0.6521\n",
            "Epoch 9/10\n",
            "746/746 [==============================] - 26s 35ms/step - loss: 0.7516 - accuracy: 0.6505 - val_loss: 0.7502 - val_accuracy: 0.6522\n",
            "Epoch 10/10\n",
            "746/746 [==============================] - 24s 32ms/step - loss: 0.7512 - accuracy: 0.6505 - val_loss: 0.7476 - val_accuracy: 0.6521\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7d024ae67ee0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model"
      ],
      "metadata": {
        "id": "T8N7sxHk1cnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_cnn_model.save(\"text_CNNacc72.h5\")\n",
        "# Load the pre-trained TextCNN model\n",
        "pretrained_model = load_model(\"text_CNNacc72.h5\")\n",
        "\n",
        "# Remove the last layer (output layer) from the pre-trained model\n",
        "#pretrained_model.layers.pop()\n"
      ],
      "metadata": {
        "id": "d7WZ7cit1gIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "text_cnn with batch normalization"
      ],
      "metadata": {
        "id": "Ebq5cAj7pNou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, Conv1D, MaxPooling1D, Flatten, Dense, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "\n",
        "# Load and preprocess your dataset\n",
        "\n",
        "# Define vocabulary, sequences, and other parameters\n",
        "\n",
        "# Build the TextCNN model with Batch Normalization\n",
        "input_layer = Input(shape=(max_sequence_length,))\n",
        "embedding_layer = Embedding(input_dim=vocabulary_size, output_dim=embedding_dim)(input_layer)\n",
        "\n",
        "# Add Convolutional and BatchNormalization layers\n",
        "convolutional_layers = []\n",
        "\n",
        "for filter_size in filter_sizes:\n",
        "    conv_layer = Conv1D(filters=num_filters, kernel_size=filter_size, activation='relu')(embedding_layer)\n",
        "    batch_norm_layer = BatchNormalization()(conv_layer)  # Adding Batch Normalization\n",
        "    pool_layer = MaxPooling1D(pool_size=max_sequence_length - filter_size + 1)(batch_norm_layer)\n",
        "    convolutional_layers.append(pool_layer)\n",
        "\n",
        "merged_layer = tf.keras.layers.concatenate(convolutional_layers, axis=1)\n",
        "flatten_layer = Flatten()(merged_layer)\n",
        "\n",
        "# Add BatchNormalization to the Dense layers\n",
        "dense_layer = Dense(units=hidden_units, activation='relu')(flatten_layer)\n",
        "batch_norm_dense = BatchNormalization()(dense_layer)  # Adding Batch Normalization\n",
        "output_layer = Dense(units=num_classes, activation='softmax')(batch_norm_dense)\n",
        "\n",
        "# Create the model\n",
        "text_cnn_model_with_bn = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compile the model\n",
        "text_cnn_model_with_bn.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "batch_size = 64\n",
        "num_epochs = 10\n",
        "\n",
        "text_cnn_model_with_bn.fit(padded_sequences, encoded_labels, batch_size=batch_size, epochs=num_epochs, validation_split=0.2)"
      ],
      "metadata": {
        "id": "oSW_7859cF0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "data Perturbation"
      ],
      "metadata": {
        "id": "3Dij5PLzpVG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def apply_keyword_avoidance_perturbation(original_sample, keyword_library):\n",
        "    def randomize_number(n):\n",
        "        return str(np.random.randint(10))\n",
        "\n",
        "    def randomize_letter(c):\n",
        "        if c.isalpha():\n",
        "            return chr(np.random.randint(ord('a'), ord('z') + 1))\n",
        "        return c\n",
        "\n",
        "    perturbed_words = []\n",
        "    for word in original_sample:\n",
        "        if word in keyword_library:\n",
        "            perturbed_words.append(word)\n",
        "        else:\n",
        "            special_characters = ['\\'', '\"', '<', '>', '+', '-', '_', '*', '=', '{', '}', '(', ')', '[', ']', 'âˆ¼', '/',\n",
        "                                  '\\\\', '#', ':', ';', '?', '!', '&', '@', '%']\n",
        "            if any(char in special_characters for char in word):\n",
        "                perturbed_words.append(word)\n",
        "            else:\n",
        "                perturbed_word = ''.join(randomize_number(c) if c.isdigit() else randomize_letter(c) for c in word)\n",
        "                perturbed_words.append(perturbed_word)\n",
        "\n",
        "    return perturbed_words\n",
        "\n",
        "# Define the keyword library\n",
        "keyword_library = {\"login\", \"password\", \"malicious\", \"attack\", \"exploit\", \"hacker\", \"virus\",\n",
        "                   \"SELECT\", \"UNION\", \"AND\", \"http\", \"script\"}\n",
        "\n",
        "# Example usage of data augmentation with payload\n",
        "original_payload = tokenized_payloads[0]  # Use any payload from your tokenized_payloads list\n",
        "\n",
        "augmented_payload = apply_keyword_avoidance_perturbation(original_payload, keyword_library)\n",
        "\n",
        "print(\"Original Payload:\", original_payload)\n",
        "print(\"Augmented Payload:\", augmented_payload)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hU8LUVNWR28q",
        "outputId": "7b16a87e-73e3-4eef-ddfd-b42b44a27bd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Payload: ['nan']\n",
            "Augmented Payload: ['npy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def apply_keyword_avoidance_perturbation(original_url, keyword_library):\n",
        "    def randomize_number(n):\n",
        "        return str(np.random.randint(10))\n",
        "\n",
        "    def randomize_letter(c):\n",
        "        if c.isalpha():\n",
        "            return chr(np.random.randint(ord('a'), ord('z') + 1))\n",
        "        return c\n",
        "\n",
        "    # Split the original URL into components using \"/\", \"=\", and \".\"\n",
        "    components = re.split(r'([/=\\.&?])', original_url)\n",
        "    perturbed_components = []\n",
        "\n",
        "    for i, component in enumerate(components):\n",
        "        if component in ('/', '=', '.',\"&\",\"?\"):\n",
        "            perturbed_components.append(component)\n",
        "            continue\n",
        "\n",
        "        if any(keyword in component for keyword in keyword_library):\n",
        "            perturbed_components.append(component)\n",
        "        else:\n",
        "            perturbed_component = ''.join(randomize_number(c) if c.isdigit() else randomize_letter(c) for c in component)\n",
        "            perturbed_components.append(perturbed_component)\n",
        "\n",
        "    # Join the perturbed components back into the URL using original separators\n",
        "    perturbed_url = ''.join(perturbed_components)\n",
        "    return perturbed_url\n",
        "\n",
        "# Define the keyword library\n",
        "keyword_library = {\"login\",\"user\", \"password\", \"malicious\", \"attack\", \"exploit\", \"hacker\", \"virus\",\n",
        "                   \"com\", \"co\", \"cn\", \"in\", \"http\",\"php\"}\n",
        "\n",
        "import re  # Import the re module for regular expressions\n",
        "\n",
        "# Example usage of data augmentation with URL\n",
        "original_url = \"http://example.com/login.php?user=admin&password=123456\"\n",
        "\n",
        "augmented_url = apply_keyword_avoidance_perturbation(original_url, keyword_library)\n",
        "\n",
        "print(\"Original URL:\", original_url)\n",
        "print(\"Augmented URL:\", augmented_url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDJWIrt5Wyna",
        "outputId": "d1222bf2-00d9-48f6-af61-7beb964f304a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original URL: http://example.com/login.php?user=admin&password=123456\n",
            "Augmented URL: http://zupinsl.com/login.php?user=admin&password=853194\n"
          ]
        }
      ]
    }
  ]
}